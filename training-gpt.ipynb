{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":8866107,"sourceType":"datasetVersion","datasetId":5336142}],"dockerImageVersionId":30732,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-07-16T04:07:08.298516Z","iopub.execute_input":"2024-07-16T04:07:08.298921Z","iopub.status.idle":"2024-07-16T04:07:09.745141Z","shell.execute_reply.started":"2024-07-16T04:07:08.298879Z","shell.execute_reply":"2024-07-16T04:07:09.743802Z"},"trusted":true},"outputs":[{"name":"stdout","text":"/kaggle/input/shakespere-works/input.txt\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"with open('/kaggle/input/shakespere-works/input.txt','r',encoding='utf-8') as f:\n    text=f.read()","metadata":{"execution":{"iopub.status.busy":"2024-07-16T04:07:09.747267Z","iopub.execute_input":"2024-07-16T04:07:09.747790Z","iopub.status.idle":"2024-07-16T04:07:09.775318Z","shell.execute_reply.started":"2024-07-16T04:07:09.747758Z","shell.execute_reply":"2024-07-16T04:07:09.773951Z"},"trusted":true},"outputs":[],"execution_count":2},{"cell_type":"code","source":"print('len in text dataset',len(text))","metadata":{"execution":{"iopub.status.busy":"2024-07-16T04:07:09.776695Z","iopub.execute_input":"2024-07-16T04:07:09.777057Z","iopub.status.idle":"2024-07-16T04:07:09.783945Z","shell.execute_reply.started":"2024-07-16T04:07:09.777027Z","shell.execute_reply":"2024-07-16T04:07:09.782405Z"},"trusted":true},"outputs":[{"name":"stdout","text":"len in text dataset 1115394\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"chars=sorted(list(set(text)))\nvocab_size=len(chars)\nprint(''.join(chars))\nprint(vocab_size)","metadata":{"execution":{"iopub.status.busy":"2024-07-16T04:07:09.786627Z","iopub.execute_input":"2024-07-16T04:07:09.787028Z","iopub.status.idle":"2024-07-16T04:07:09.816988Z","shell.execute_reply.started":"2024-07-16T04:07:09.786984Z","shell.execute_reply":"2024-07-16T04:07:09.815600Z"},"trusted":true},"outputs":[{"name":"stdout","text":"\n !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n65\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"stoi={ch:i for i,ch in enumerate(chars)}\nitos={i:ch for i,ch in enumerate(chars)}\n\n\nencode=lambda s:[stoi[c] for c in s]\ndecode=lambda l:''.join([itos[i] for i in l])\n\nprint(encode('hii there'))\nprint(decode(encode('hii there')))","metadata":{"execution":{"iopub.status.busy":"2024-07-16T04:07:09.818526Z","iopub.execute_input":"2024-07-16T04:07:09.818985Z","iopub.status.idle":"2024-07-16T04:07:09.829391Z","shell.execute_reply.started":"2024-07-16T04:07:09.818942Z","shell.execute_reply":"2024-07-16T04:07:09.827923Z"},"trusted":true},"outputs":[{"name":"stdout","text":"[46, 47, 47, 1, 58, 46, 43, 56, 43]\nhii there\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"import torch\ndata=torch.tensor(encode(text),dtype=torch.long)\n","metadata":{"execution":{"iopub.status.busy":"2024-07-16T04:07:09.830888Z","iopub.execute_input":"2024-07-16T04:07:09.831682Z","iopub.status.idle":"2024-07-16T04:07:14.754898Z","shell.execute_reply.started":"2024-07-16T04:07:09.831648Z","shell.execute_reply":"2024-07-16T04:07:14.753506Z"},"trusted":true},"outputs":[],"execution_count":6},{"cell_type":"code","source":"n=int(0.9*len(data))\ntrain_data=data[:n]\nval_data=data[n:]","metadata":{"execution":{"iopub.status.busy":"2024-07-16T04:07:14.756575Z","iopub.execute_input":"2024-07-16T04:07:14.757179Z","iopub.status.idle":"2024-07-16T04:07:14.783606Z","shell.execute_reply.started":"2024-07-16T04:07:14.757123Z","shell.execute_reply":"2024-07-16T04:07:14.781772Z"},"trusted":true},"outputs":[],"execution_count":7},{"cell_type":"code","source":"len(train_data)\n","metadata":{"execution":{"iopub.status.busy":"2024-07-16T04:07:14.785657Z","iopub.execute_input":"2024-07-16T04:07:14.786100Z","iopub.status.idle":"2024-07-16T04:07:14.806656Z","shell.execute_reply.started":"2024-07-16T04:07:14.786067Z","shell.execute_reply":"2024-07-16T04:07:14.805400Z"},"trusted":true},"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"1003854"},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"torch.manual_seed(1337)\nbatch_size=4\nblock_size=8\n\ndef get_batch(split):\n    data=train_data if split=='train' else val_data\n    ix=torch.randint(len(data)-block_size,(batch_size,))\n    x=torch.stack([data[i:i+block_size] for i in ix])\n    y=torch.stack([data[i+1:i+block_size+1] for i in ix])\n    return x,y\n\nxb,yb=get_batch('train')\nprint('x_train')\nprint(xb.shape)\nprint(xb)\nprint(yb.shape)\nprint(yb)\n\n\nprint('------')\nfor b in range(batch_size):\n    for t in range(block_size):\n\n        context=xb[b,:t+1]\n        \n        \n        target=yb[b,t]\n        print(f'when input is {context.tolist()} the target:{target}')","metadata":{"execution":{"iopub.status.busy":"2024-07-16T04:07:14.808621Z","iopub.execute_input":"2024-07-16T04:07:14.809850Z","iopub.status.idle":"2024-07-16T04:07:14.881398Z","shell.execute_reply.started":"2024-07-16T04:07:14.809814Z","shell.execute_reply":"2024-07-16T04:07:14.880182Z"},"trusted":true},"outputs":[{"name":"stdout","text":"x_train\ntorch.Size([4, 8])\ntensor([[24, 43, 58,  5, 57,  1, 46, 43],\n        [44, 53, 56,  1, 58, 46, 39, 58],\n        [52, 58,  1, 58, 46, 39, 58,  1],\n        [25, 17, 27, 10,  0, 21,  1, 54]])\ntorch.Size([4, 8])\ntensor([[43, 58,  5, 57,  1, 46, 43, 39],\n        [53, 56,  1, 58, 46, 39, 58,  1],\n        [58,  1, 58, 46, 39, 58,  1, 46],\n        [17, 27, 10,  0, 21,  1, 54, 39]])\n------\nwhen input is [24] the target:43\nwhen input is [24, 43] the target:58\nwhen input is [24, 43, 58] the target:5\nwhen input is [24, 43, 58, 5] the target:57\nwhen input is [24, 43, 58, 5, 57] the target:1\nwhen input is [24, 43, 58, 5, 57, 1] the target:46\nwhen input is [24, 43, 58, 5, 57, 1, 46] the target:43\nwhen input is [24, 43, 58, 5, 57, 1, 46, 43] the target:39\nwhen input is [44] the target:53\nwhen input is [44, 53] the target:56\nwhen input is [44, 53, 56] the target:1\nwhen input is [44, 53, 56, 1] the target:58\nwhen input is [44, 53, 56, 1, 58] the target:46\nwhen input is [44, 53, 56, 1, 58, 46] the target:39\nwhen input is [44, 53, 56, 1, 58, 46, 39] the target:58\nwhen input is [44, 53, 56, 1, 58, 46, 39, 58] the target:1\nwhen input is [52] the target:58\nwhen input is [52, 58] the target:1\nwhen input is [52, 58, 1] the target:58\nwhen input is [52, 58, 1, 58] the target:46\nwhen input is [52, 58, 1, 58, 46] the target:39\nwhen input is [52, 58, 1, 58, 46, 39] the target:58\nwhen input is [52, 58, 1, 58, 46, 39, 58] the target:1\nwhen input is [52, 58, 1, 58, 46, 39, 58, 1] the target:46\nwhen input is [25] the target:17\nwhen input is [25, 17] the target:27\nwhen input is [25, 17, 27] the target:10\nwhen input is [25, 17, 27, 10] the target:0\nwhen input is [25, 17, 27, 10, 0] the target:21\nwhen input is [25, 17, 27, 10, 0, 21] the target:1\nwhen input is [25, 17, 27, 10, 0, 21, 1] the target:54\nwhen input is [25, 17, 27, 10, 0, 21, 1, 54] the target:39\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\ntorch.manual_seed(1337)\n\nclass BigramLanguageModel(nn.Module):\n    \n    def __init__(self,vocab_size):\n        super().__init__()\n        self.token_embedding_table=nn.Embedding(vocab_size,vocab_size)\n        \n    def forward(self,idx,targets=None):\n        logits=self.token_embedding_table(idx)\n        \n        if targets is None:\n            loss=None\n            \n        else:\n        \n            B,T,C=logits.shape\n            logits=logits.view(B*T,C)\n            targets=targets.view(B*T)\n            loss=F.cross_entropy(logits,targets)\n            \n        return logits,loss\n        \n        \n    def generate(self,idx,max_new_tokens):\n        for _ in range(max_new_tokens):\n            logits,loss=self(idx)\n            logits=logits[:,-1,:]\n\n            probs=F.softmax(logits,dim=1)\n            \n            idx_next=torch.multinomial(probs,num_samples=1)\n            \n            idx=torch.cat((idx,idx_next),dim=1)\n            \n        return idx\n","metadata":{"execution":{"iopub.status.busy":"2024-07-16T04:07:14.896641Z","iopub.execute_input":"2024-07-16T04:07:14.897002Z","iopub.status.idle":"2024-07-16T04:07:14.911684Z","shell.execute_reply.started":"2024-07-16T04:07:14.896971Z","shell.execute_reply":"2024-07-16T04:07:14.910509Z"},"trusted":true},"outputs":[],"execution_count":11},{"cell_type":"code","source":"m=BigramLanguageModel(vocab_size)\nlogits,loss=m(xb,yb)\nprint(logits.shape)\nprint(loss)\n\n\nprint(decode(m.generate(idx=torch.zeros((1,1),dtype=torch.long)\n,max_new_tokens=100)[0].tolist()))","metadata":{"execution":{"iopub.status.busy":"2024-07-16T04:07:14.913226Z","iopub.execute_input":"2024-07-16T04:07:14.913590Z","iopub.status.idle":"2024-07-16T04:07:15.044106Z","shell.execute_reply.started":"2024-07-16T04:07:14.913560Z","shell.execute_reply":"2024-07-16T04:07:15.042790Z"},"trusted":true},"outputs":[{"name":"stdout","text":"torch.Size([32, 65])\ntensor(4.8786, grad_fn=<NllLossBackward0>)\n\nSr?qP-QWktXoL&jLDJgOLVz'RIoDqHdhsV&vLLxatjscMpwLERSPyao.qfzs$Ys$zF-w,;eEkzxjgCKFChs!iWW.ObzDnxA Ms$3\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"optimizer=torch.optim.Adam(m.parameters(),lr=1e-3)","metadata":{"execution":{"iopub.status.busy":"2024-07-16T04:07:15.045594Z","iopub.execute_input":"2024-07-16T04:07:15.046086Z","iopub.status.idle":"2024-07-16T04:07:16.749504Z","shell.execute_reply.started":"2024-07-16T04:07:15.046045Z","shell.execute_reply":"2024-07-16T04:07:16.748013Z"},"trusted":true},"outputs":[],"execution_count":13},{"cell_type":"code","source":"batch_size=32\n\nfor steps in range(10000):\n    xb,yb=get_batch('train')\n    logits,loss=m(xb,yb)\n    optimizer.zero_grad(set_to_none=True)\n    loss.backward()\n    optimizer.step()\n    \nprint(loss.item())","metadata":{"execution":{"iopub.status.busy":"2024-07-16T04:07:16.751232Z","iopub.execute_input":"2024-07-16T04:07:16.751773Z","iopub.status.idle":"2024-07-16T04:07:37.133054Z","shell.execute_reply.started":"2024-07-16T04:07:16.751740Z","shell.execute_reply":"2024-07-16T04:07:37.131491Z"},"trusted":true},"outputs":[{"name":"stdout","text":"2.572469472885132\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"print(decode(m.generate(idx=torch.zeros((1,1),dtype=torch.long)\n,max_new_tokens=500)[0].tolist()))","metadata":{"execution":{"iopub.status.busy":"2024-07-16T04:07:37.134548Z","iopub.execute_input":"2024-07-16T04:07:37.134929Z","iopub.status.idle":"2024-07-16T04:07:37.212496Z","shell.execute_reply.started":"2024-07-16T04:07:37.134895Z","shell.execute_reply":"2024-07-16T04:07:37.210744Z"},"trusted":true},"outputs":[{"name":"stdout","text":"\nIyoteng h hasbe pave pirance\nRie hicomyonthar's\nPlinseard ith henoure wounonthioneir thondy, y heltieiengerofo'dsssit ey\nKIN d pe wither vouprrouthercckehathe; d!\nMy hind tt hinig t ouchos tes; st yo hind wotte grotonear 'so it t jod weancotha:\nh hay.JUCle n prids, r loncave w hollular s O:\nHIs; ht anjx?\n\nDUThineent.\n\nLavinde.\nathave l.\nKEONGBUCHandspo be y,-hedarwnoddy scace, tridesar, wne'shenous s ls, theresseys\nPlorseelapinghiybHen yof GLUCEN t l-t E:\nI hisgothers w dere! ABer wotouciullle's\n","output_type":"stream"}],"execution_count":15},{"cell_type":"markdown","source":"# The mathematical trick in self-attention","metadata":{}},{"cell_type":"code","source":"torch.manual_seed(1337)\nB,T,C=4,8,2\nx=torch.randn(B,T,C)\nprint(x.shape)","metadata":{"execution":{"iopub.status.busy":"2024-07-16T04:31:43.742204Z","iopub.execute_input":"2024-07-16T04:31:43.742644Z","iopub.status.idle":"2024-07-16T04:31:43.751492Z","shell.execute_reply.started":"2024-07-16T04:31:43.742614Z","shell.execute_reply":"2024-07-16T04:31:43.749827Z"},"trusted":true},"outputs":[{"name":"stdout","text":"torch.Size([4, 8, 2])\n","output_type":"stream"}],"execution_count":41},{"cell_type":"code","source":"xbow=torch.zeros((B,T,C))\nfor b in range(B):\n    for t in range(T):\n        xprev=x[b,:t+1]\n        xbow[b,t]=torch.mean(xprev,0)\n    \n        ","metadata":{"execution":{"iopub.status.busy":"2024-07-16T04:35:32.036215Z","iopub.execute_input":"2024-07-16T04:35:32.036622Z","iopub.status.idle":"2024-07-16T04:35:32.045264Z","shell.execute_reply.started":"2024-07-16T04:35:32.036589Z","shell.execute_reply":"2024-07-16T04:35:32.044035Z"},"trusted":true},"outputs":[],"execution_count":54},{"cell_type":"code","source":"tn=torch.randn(4,8,32)\nsi=torch.randn()","metadata":{"execution":{"iopub.status.busy":"2024-07-16T06:02:50.074673Z","iopub.execute_input":"2024-07-16T06:02:50.075111Z","iopub.status.idle":"2024-07-16T06:02:50.081833Z","shell.execute_reply.started":"2024-07-16T06:02:50.075077Z","shell.execute_reply":"2024-07-16T06:02:50.080279Z"},"trusted":true},"outputs":[],"execution_count":69},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"wei=torch.tril(torch.ones(T,T))\nwei=wei/wei.sum(1,keepdim=True)\n\nxbow2=wei@x \nprint(xbow2.shape)\nprint(xbow.shape)\ntorch.allclose(xbow,xbow2)","metadata":{"execution":{"iopub.status.busy":"2024-07-16T04:35:32.333411Z","iopub.execute_input":"2024-07-16T04:35:32.334753Z","iopub.status.idle":"2024-07-16T04:35:32.344905Z","shell.execute_reply.started":"2024-07-16T04:35:32.334652Z","shell.execute_reply":"2024-07-16T04:35:32.343500Z"},"trusted":true},"outputs":[{"name":"stdout","text":"torch.Size([4, 8, 2])\ntorch.Size([4, 8, 2])\n","output_type":"stream"},{"execution_count":55,"output_type":"execute_result","data":{"text/plain":"False"},"metadata":{}}],"execution_count":55},{"cell_type":"code","source":"tril=torch.tril(torch.ones(T,T))\nwei=torch.zeros((T,T))\nwei=wei.masked_fill(tril==0,float('-inf'))\nwei=F.softmax(wei,dim=-1)\nxbow3=wei@x\ntorch.allclose(xbow2,xbow3)","metadata":{"execution":{"iopub.status.busy":"2024-07-16T04:43:42.968554Z","iopub.execute_input":"2024-07-16T04:43:42.969000Z","iopub.status.idle":"2024-07-16T04:43:42.979309Z","shell.execute_reply.started":"2024-07-16T04:43:42.968966Z","shell.execute_reply":"2024-07-16T04:43:42.978062Z"},"trusted":true},"outputs":[{"execution_count":62,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":62},{"cell_type":"code","source":"# Define your input tensor (logits)\ninputs = torch.tensor([[0.0, 0.2, 0.3]])  # Shape (batch_size, num_classes)\ninputs=torch.nn.functional.softmax(inputs)\nprint(inputs)\n# Define your target tensor (labels)\ntarget = torch.tensor([1])  # Can be class indices (LongTensor)\n\n# Calculate the loss\nloss = F.cross_entropy(inputs, target)\n\n# Print the loss value\nprint(loss)","metadata":{"execution":{"iopub.status.busy":"2024-07-16T04:07:37.213801Z","iopub.execute_input":"2024-07-16T04:07:37.214136Z","iopub.status.idle":"2024-07-16T04:07:37.224862Z","shell.execute_reply.started":"2024-07-16T04:07:37.214109Z","shell.execute_reply":"2024-07-16T04:07:37.223606Z"},"trusted":true},"outputs":[{"name":"stdout","text":"tensor([[0.2800, 0.3420, 0.3780]])\ntensor(1.0908)\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_33/1462533493.py:3: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n  inputs=torch.nn.functional.softmax(inputs)\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\n#from torch.nn import FeedForward\n\nbatch_size=32\nblock_size=8\nmax_iters=5000\neval_interval=300\nlearning_rate=1e-3\ndevice='cuda' if torch.cuda.is_available() else 'cpu'\neval_iters=200\nn_embd=384\nn_head=6\nn_layer=6\ndropout=0.2\n\ntorch.manual_seed(1337)\n\nwith open(r\"/kaggle/input/shakespere-works/input.txt\",'r',encoding='utf-8') as f:\n\n\n    text=f.read()\n\nchars=sorted(list(set(text)))\nvocab_size=len(chars)\n\n\nstoi={ch:i for i,ch in enumerate(chars)}\nitos={i:ch for i,ch in enumerate(chars)}\n\nencode=lambda s:[stoi[c] for c in s]\ndecode=lambda l:[''.join(itos[i] for i in l)]\n\n\ndata=torch.tensor(encode(text),dtype=torch.long)\nn=int(0.9*len(data))\ntrain_data=data[:n]\nval_data=data[n:]\n\n\ndef get_batch(split):\n    data=train_data if split == 'train' else val_data\n    ix=torch.randint(len(data)-block_size,(batch_size,))\n    x=torch.stack([data[i:i+block_size] for i in ix])\n    y=torch.stack([data[i+1:i+block_size+1] for i in ix])\n    x,y=x.to(device),y.to(device)\n    return x,y\n\n@torch.no_grad()\ndef estimate_loss():\n    out={}\n    model.eval()\n    for split in ['train','val']:\n        losses=torch.zeros(eval_iters)\n        for k in range(eval_iters):\n            X,Y=get_batch(split)\n            logits,loss=model(X,Y)\n            losses[k]=loss.item()\n        out[split]=losses.mean()\n    model.train()\n    return out\n\nclass Head(nn.Module):\n    # one head of self-attention\n\n    def __init__(self,head_size):\n        super(Head,self).__init__()\n        self.key=nn.Linear(n_embd,head_size,bias=False)\n        self.query=nn.Linear(n_embd,head_size,bias=False)\n        self.value=nn.Linear(n_embd,head_size,bias=False)\n\n        self.register_buffer('trill',torch.tril(torch.ones(block_size,block_size)))\n        self.dropout=nn.Dropout(dropout)\n\n    def forward(self,x):\n        B,T,C=x.shape\n        k=self.key(x)\n        q=self.query(x)\n\n        wei=q@k.transpose(-2,-1)*C**-0.5\n        wei=wei.masked_fill(self.trill[:T,:T]==0,float('-inf'))\n        wei=F.softmax(wei,dim=-1)\n        wei=self.dropout(wei)\n\n        v=self.value(x)\n        out=wei@v\n        return out\n\n\n\nclass MultiHeadAttention(nn.Module):\n    \"\"\"Multiple heads of self attention in parallel\"\"\"\n\n    def __init__(self,num_heads,head_size):\n        super(MultiHeadAttention,self).__init__()\n        self.heads=nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n        self.proj=nn.Linear(n_embd,n_embd)\n        self.dropout=nn.Dropout(dropout)\n    def forward(self,x):\n        out= torch.cat([h(x) for h in self.heads],dim=-1)\n\n        out=self.dropout(self.proj(out))\n\n        return out\n\nclass FeedForward(nn.Module):\n\n    def __init__(self,n_embd):\n        super(FeedForward,self).__init__()\n        self.net=nn.Sequential(\n            nn.Linear(n_embd,4*n_embd),\n            nn.ReLU(),\n            nn.Linear(4*n_embd,n_embd),\n            nn.Dropout(dropout),\n        )\n\n    def forward(self,x):\n        return self.net(x)\n\n\nclass Block(nn.Module):\n    \"\"\" Transformer block: communication followed by computation\"\"\"\n\n    def __init__(self,n_embd,n_head):\n        super(Block,self).__init__()\n        head_size=n_embd//n_head\n        self.sa=MultiHeadAttention(n_head,head_size)\n        self.ffwd=FeedForward(n_embd)\n        self.ln1=nn.LayerNorm(n_embd)\n        self.ln2=nn.LayerNorm(n_embd)\n\n    def forward(self,x):\n        x=x+self.sa(self.ln1(x))\n\n        x=x+self.ffwd(self.ln2(x))\n        return x\n\n\n\n\nclass BigramLanguageModel(nn.Module):\n\n    def __init__(self):\n        super().__init__()\n        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n        self.position_embedding_table = nn.Embedding(block_size,n_embd)\n        self.blocks=nn.Sequential(*[Block(n_embd,n_head=n_head) for _ in range(n_layer)])\n        self.ln_f=nn.LayerNorm(n_embd)\n        self.lm_head=nn.Linear(n_embd,vocab_size)\n\n    def forward(self, idx, targets=None):\n        B,T=idx.shape\n\n        tok_emb = self.token_embedding_table(idx)\n        pos_emb = self.position_embedding_table(torch.arange(T,device=device))\n        x=tok_emb+pos_emb\n        x=self.blocks(x)\n        x=self.ln_f(x)\n        logits=self.lm_head(x)#(B,T, vocab_size)\n\n        if targets is None:\n            loss = None\n\n        else:\n\n            B, T, C = logits.shape\n            logits = logits.view(B * T, C)\n\n\n            targets = targets.view(B * T,)\n            loss = F.cross_entropy(logits, targets)\n\n        return logits, loss\n\n    def generate(self, idx, max_new_tokens):\n        for _ in range(max_new_tokens):\n\n            idx_cond=idx[:,-block_size:]\n            logits, loss = self(idx_cond)\n            logits = logits[:, -1, :]\n\n            probs = F.softmax(logits, dim=1)\n\n            idx_next = torch.multinomial(probs, num_samples=1)\n\n            idx = torch.cat((idx, idx_next), dim=1)\n\n        return idx\n\n\nmodel=BigramLanguageModel()\nm=model.to(device)\n\noptimizer=torch.optim.Adam(model.parameters(),learning_rate)\n\nfor iter in range (max_iters):\n     if iter % eval_interval==0:\n        losses=estimate_loss()\n        print(f\"step{iter}: train loss {losses['train']:.4f},val loss {losses['val']:.4f}\")\n\n\n     xb,yb=get_batch('train')\n     logits,loss=model(xb,yb)\n     optimizer.zero_grad(set_to_none=True)\n     loss.backward()\n     optimizer.step()\n\n\ncontext=torch.zeros((1,1),dtype=torch.long,device=device)\nprint(decode(m.generate(context,max_new_tokens=500)[0].tolist()))\n\n\n\n\n","metadata":{"execution":{"iopub.status.busy":"2024-07-16T12:38:29.416716Z","iopub.execute_input":"2024-07-16T12:38:29.417064Z","iopub.status.idle":"2024-07-16T12:45:00.500107Z","shell.execute_reply.started":"2024-07-16T12:38:29.417036Z","shell.execute_reply":"2024-07-16T12:45:00.498899Z"},"trusted":true},"outputs":[{"name":"stdout","text":"step0: train loss 4.3633,val loss 4.3610\nstep300: train loss 2.3067,val loss 2.3192\nstep600: train loss 2.1691,val loss 2.2275\nstep900: train loss 2.1296,val loss 2.1878\nstep1800: train loss 2.0120,val loss 2.1390\nstep2700: train loss 1.9678,val loss 2.0727\nstep3000: train loss 1.9311,val loss 2.0419\nstep3300: train loss 1.9328,val loss 2.0557\nstep3600: train loss 1.9129,val loss 2.0277\nstep3900: train loss 1.8953,val loss 2.0055\nstep4200: train loss 1.8926,val loss 2.0103\nstep4500: train loss 1.8675,val loss 1.9924\nstep4800: train loss 1.8689,val loss 1.9874\n[\"\\nBale here.\\n\\nNo, fice it, she pregue\\nRome I mer shonoule talk. What his fortle he susblors;\\nHoush to have unidound is of, how yet the descian, by have befur deyears;\\nWhich perison the know true crue aloun deay,\\nAnd it see fierfices: avost the heart your ress of o munt your fatest?\\nCome mabe fathers will.\\n\\nIS!'\\nAh, refore wife, was paciinna;\\nChillows thy grow by-treought,\\nReetomes! anorenfnoe, whis.\\n\\nMBRUTUS:\\nA kere horks'd the wome subjering friends ways?\\n\\nDUKE VIO:\\nYork the this man's hate, be m\"]\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(decode(m.generate(idx=torch.zeros((1,1),dtype=torch.long,device=device),max_new_tokens=5000)[0].tolist()))","metadata":{"execution":{"iopub.status.busy":"2024-07-16T12:55:04.265028Z","iopub.execute_input":"2024-07-16T12:55:04.265459Z","iopub.status.idle":"2024-07-16T12:56:47.179981Z","shell.execute_reply.started":"2024-07-16T12:55:04.265415Z","shell.execute_reply":"2024-07-16T12:56:47.178799Z"},"trusted":true},"outputs":[{"name":"stdout","text":"[\"\\nShall Godly pear\\nI tare kintruse\\na thee frould, but to and thy cous helps,\\nIs to come, the person;\\nAnd\\nThe brough sengue, wan exseak a of Mornides,\\nBe to fave you do chay\\nMy lighte and you kin one stay sonder's, nursest us\\nfrough ass'll be\\nshicheres, I nall fawn letio be fen wo ince, 'AP I somen: you?\\n\\nMieped,\\nDo movere and in yor syounds.\\n\\nKh Tis rayaineves sir:\\nThat it thee adverice from all ant I is a me, to his behe conse croffatess?\\nChall press, ever shalt\\nI strubles.\\n\\nAUf Yorn agaid!\\n\\nSey his say, on this what I voust try, it ead With'd you know fem, the holdiy resomeiu.\\n\\nJULIET:\\nI with see-fold,\\nbuther cany drecest's of I honour that of that sheetmen day, intry, there though hus acconduty.\\n\\nKING HIRA\\nD:\\nChave true r, yourhly peather\\nWhat more,\\nIf that Capter,\\nThese orr'd do entrare for my consisodise, should; the wish'd son.\\nA thy: sis, nivenre's saven'st is dead aster the\\ngivins.\\nBut this thou,\\nWill a shath friend o'er of a maidison;\\nBut now to sidistrytings\\nSay brough a prsue;\\nAnd\\nWere a imon should prevough no with no sove the decuslonkin:\\nBut my lear py-boly! a naye,\\nAnd who know's can's lave ying be that\\nAnse.\\nComen degs ta!\\n\\nAGELOUCESTER:\\nPlown, which this sliepiand's is warwors\\nHe weeph.\\n\\nMARGAEY:\\nHapoor Glourans, and beague he canse had, Horse to the have full's of that an that sted fagiently more sons,\\nAlad thy iu\\nhis diprance hoped argonex, doen\\nwhy,\\nHigh ven\\nBans.\\n\\nCome mishortal;\\nAre whoss hee speluke will, my mattin the liff,\\nAnd my deepherns, he fating\\nSoft, whome dayfriath\\nWill what;\\n\\nBounnot red the wards! Pear, in his this I ston;\\nOr but agaall.\\nWhy he cany ginks,\\nAnd and you, all. \\nFirs, for umpecont suwn.\\n\\nQUEEN ELLO:\\nNor your.\\n\\nGLOUCESTER:\\nI do behom, ge tonguot,\\nchear ronght\\nWhich,\\nBut that us on that for's us too know yaar,\\nI wife.\\n\\nKING EDWARD IV:\\nkind.\\nGodsome wend py horou, man\\nbut fly affood whill red.\\n\\nPOLISTER:\\nOf you have catious, as dinesh tho'grefroareds thee? O, so my rachiaso and\\nwilt you to grieve bear hagfffe leffore murta that ginist:\\nAy, which them pass: a pray allo here to me high kigh but the cut name sputy, whose you thou have fo, Secold grece trances insunjoinom, so whese: do bus' en this, is nevered\\nwe double\\nBUgh this make\\nHence chalfst;\\nIf have dess, 'Tis was's.\\nEver; pulse him?\\nBut is feepeens,\\nWhere daidnat on fecilin me suilipusbass'd you arcond the Vearth, i whounth then.\\n\\nSecry'd charse wife zencousion:\\nHow, I noom when, for brood; in which as is lady chill the paster nather will,\\nConvance or helhful peact, medir my but where Mall, where,\\nThonourn'd.\\n\\nCORIANIUS:\\nChere gentell-capuly have per the him, and weave thee our very axearthed,\\nAnd, love,\\nAnd the parvain marce, balling\\nconsutantle marry callan the king.\\n\\nWARWARD I:\\nAy, the we kingeland,\\nAnd stay me willl, iyoun Play not poor graviol, will us a moush, Come: I know the fatuiloy wradent panusintell ans graca' house have forse Ento coust,\\nBut abure on mustrow and your k'd?\\n\\nGRESS OF YORK:\\nAlrow and glike crow'd f.\\n\\nFRIARE:\\nAnd Edvis seek, me\\nEightss' the shall bwe is men, but is talks, with many this thanse your lords I now grip here may,\\nI randle,\\nUS Burgh hand\\nAs wo nould skingly heaven a such'd\\nThis by stice!\\n\\nROMEO:\\nSays, nevill imake or this from now sreces more the wishrouck'd's thou sweam\\nWho in consurres'daftling cornsreason;\\nAnd, gono pruou;\\nAlds is before have.\\n\\nGLOUCESTo werefule the sost cho, ster,\\nAnd but my from you\\nmady\\nThat kily.\\n\\nGLOUCESTER:\\nShom you, whom, my abaft!\\nWho the ut:\\nHoly which  nou'en marce?\\n\\nBUCKINGBROKE, for mort high thou, her faish leaver 's he captoraust, fate ien dirtue, your,  rough us:\\nWell, be placount.\\n\\nDUKE EDWICINA:\\nAnd a prellone. Why, a my nor good Again.\\n\\nANGELIUS:\\nI compray unjeasushnoss maken can once havy betherefore by const 'f althand!\\nCarve my fort:\\nAnd, thou cron\\ndeathy.\\n\\nGLOUCESTES:\\nUnour withat I would I am so:\\nWhis your should.\\n\\nWICHENY NIUS:\\nWhat'er's is to plee the greantint at od see, to day, see strence thee the satter; eyes, and staynipe be nost\\nThe spaif.\\n\\nDUKE VIrcian:\\nCome?\\nI, amortly before athus, that man you kind thou, and peterm king'd's a kingen is advosen'd by he him good is your we'll diver that luse! smorded's kill dery\\nAnd mate, thou beliexs; you be pleace,\\nThnst.\\n\\nDUKE VINCENTIO:\\nA one have\\nthe cepon stsuchor'd shill your,\\nOr you, the shall the ame, whom\\nhave wich ribe willorse!\\n\\nCAMILLO:\\nBehis not; bring's to place thence of thy prould\\nWather was?\\nSty, that?\\n\\nEDWARD IV:\\nButin make dishnow kis our come pred, who gooner\\nAnd hight\\nyess; heis crews,\\nBy anoner such, onoth peat? Walts!\\nThey putiousle, no chought it on for of a oy ssiree him king,\\nIn in him.\\n\\nISABENH:\\nAlbe?\\nOf is though disst less is coorseless' thall fles sou; if andger what\\nThe we not waspight my my you ba vatter's dune.\\n\\nThat, thou be\\nsay with him?\\nO be gla art thanking.\\n\\nKING HENRY VIstern:\\nAnd go, be thy recight as pusjicion.\\n\\nQUEEN nidinghip is ungercor me on the chaid balien ye\\nfare prectieness, havove grance:\\nWhere 'tis have my weition let us son?\\n\\nNor thy us\"]\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null}]}